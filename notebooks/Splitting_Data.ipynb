{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","try:\n","  %load_ext autotime\n","except:\n","  print(\"Console warning-- Autotime is jupyter platform specific\")\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import glob\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import comet_ml\n","from comet_ml import Experiment\n","import icecream as ic\n","from IPython.display import Audio, display\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torch\n","torch.cuda._initialized = True\n","import time\n","import math, random\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import init\n","from pydicom import dcmread\n","from matplotlib import pyplot as plt\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","experiment = Experiment(api_key=\"xleFjfKO3kcwc56tglgC1d3zU\",\n","                        project_name=\"Chest Xray\",log_code=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd \n","df = pd.read_csv('/media/gyasis/Drive 2/Data/vinbigdata/train.csv')\n","df.head(10)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df[['image_id', 'class_name','class_id']]\n","torch.cuda.empty_cache() "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_path(x):\n","    path_ = '/media/gyasis/Drive 2/Data/vinbigdata/train/'\n","    filetype = '.dicom'\n","    x = (path_+x+filetype)\n","    return x\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os.path\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['imagepath'] = df['image_id'].apply(lambda x: build_path(x))\n","df = df[['imagepath','class_name','class_id']]\n","df.head()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.get_dummies(df['class_name'])\n","df1 = pd.get_dummies(df['class_id'].astype(str))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#mapping for later use\n","disease= [\"Aortic enlargement\"\n",",\"Atelectasis\"\n",",\"Calcification\"\n",",\"Cardiomegaly\"\n",",\"Consolidation\"\n",",\"ILD\"\n",",\"Infiltration\"\n",",\"Lung Opacity\"\n",",\"Nodule/Mass\"\n",",\"Other lesion\"\n",",\"Pleural effusion\"\n",",\"Pleural thickening\"\n",",\"Pneumothorax\"\n",",\"Pulmonary fibrosis\"\n",",\"No_finding\"]\n","\n","#map df.class_id to disease\n","# df['class_id_test'] = df['class_id'].map(lambda x: disease[x])\n","df.head()\n","df1.columns = df1.columns.astype(int).map(lambda x: disease[x])\n","\n","s_array = np.array(df1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_class_frequencies():\n","  positive_freq = s_array.sum(axis=0) / s_array.shape[0]\n","  negative_freq = np.ones(positive_freq.shape) - positive_freq\n","  return positive_freq, negative_freq\n","\n","p,n = get_class_frequencies()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.DataFrame({\"Class\": df1.columns, \"Label\": \"Positive\", \"Value\": p})\n","data = data.append([{\"Class\": df1.columns[l], \"Label\": \"Negative\", \"Value\": v} for l, v in enumerate(n)], ignore_index=True)\n","plt.xticks(rotation=90)\n","f = sns.barplot(x=\"Class\", y=\"Value\",hue=\"Label\", data=data)\n","plt.savefig(\"skewness.png\")\n","experiment.log_image(image_data = 'skewness.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pos_weights = n\n","neg_weights = p\n","pos_contribution = p * pos_weights\n","neg_contribution = n * neg_weights\n","print(p)\n","print(n)\n","print(\"Weight to be added:  \",pos_contribution)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","data = pd.DataFrame({\"Class\": df1.columns, \"Label\": \"Positive\", \"Value\": pos_contribution})\n","data = data.append([{\"Class\": df1.columns[l], \"Label\": \"Negative\", \"Value\": v} for l, v in enumerate(neg_contribution)], ignore_index=True)\n","plt.xticks(rotation=90)\n","g = sns.barplot(x=\"Class\", y=\"Value\",hue=\"Label\", data=data)\n","plt.savefig(\"Balanced.png\")\n","experiment.log_image(image_data = \"Balanced.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision\n","from torchvision import transforms\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","c_transform = nn.Sequential(transforms.Resize([256,]), \n","                            transforms.CenterCrop(224),\n","                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)))\n","ten = torchvision.transforms.ToTensor()\n","\n","scripted_transforms = torch.jit.script(c_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = A.Compose(\n","    [A.Resize(width=256,height=256, always_apply=True),\n","                       A.HorizontalFlip(p=0.5),\n","                       A.OneOf([\n","                            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.25),\n","                            A.RandomBrightnessContrast(p=0.1, contrast_limit=0.05, brightness_limit=0.05,),\n","                            A.InvertImg(p=0.02),\n","                       ]),\n","                       A.OneOf([\n","                           A.RandomCrop(width=224, height=224, p=0.5),\n","                           A.CenterCrop(width=224, height=224, p=0.5),\n","                           \n","                       ]),\n","                       A.Resize(width=224, height=224, always_apply=True),\n","                       A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","                       ToTensorV2()\n","                    ])\n","\n","W_o_ten_transform = A.Compose(\n","    [A.Resize(width=256,height=256, always_apply=True),\n","                       A.HorizontalFlip(p=0.5),\n","                       A.OneOf([\n","                            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.25),\n","                            A.RandomBrightnessContrast(p=0.1, contrast_limit=0.05, brightness_limit=0.05,),\n","                            A.InvertImg(p=0.02),\n","                       ]),\n","                       A.OneOf([\n","                           A.RandomCrop(width=224, height=224, p=0.5),\n","                           A.CenterCrop(width=224, height=224, p=0.5),\n","                           \n","                       ]),\n","                       A.Resize(width=224, height=224, always_apply=True),\n","                       A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","                    #    ToTensorV2()\n","                    ])\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, dataset, transform=None):\n","        print('creating Dataset')\n","        self.df = dataset\n","        self.imagearray = np.asarray(dataset.imagepath)\n","        self.class_arr = np.asarray(dataset.class_id)\n","        self.transform = transform\n","        self.data_len = len(dataset.index)\n","        \n","    def __getitem__(self, index):\n","        ds = dcmread(self.imagearray[index])\n","        arr = ds.pixel_array\n","        arr = arr.astype('float')\n","        class_id = self.df.loc[index, 'class_id']\n","        arr = ten(arr)\n","        arr = arr.expand(3, -1,-1)\n","        arr = scripted_transforms(arr)\n","        \n","        return arr, class_id\n","        \n","    def __len__(self):\n","        return self.data_len\n","    \n","class AlbumentationsDataset(Dataset):\n","    def __init__(self, dataset, transform=transform):\n","        self.df = dataset\n","        self.imagearray = np.asarray(dataset.imagepath)\n","        self.class_arr = np.asarray(dataset.class_id)\n","        self.transform = transform\n","        self.data_len = len(dataset.index)\n","        \n","    def __len__(self):\n","        return self.data_len\n","    \n","    def __getitem__(self, index):\n","        \n","        ds = dcmread(self.imagearray[index])\n","        class_id = self.df.loc[index, 'class_id']\n","        arr = ds.pixel_array\n","        arr = np.stack((arr,)*3, axis=-1)\n","        arr = transform(image = arr)[\"image\"]\n","        return arr, class_id\n","class VisualDataset(Dataset):\n","    def __init__(self, dataset, transform=transform):\n","        self.df = dataset\n","        self.imagearray = np.asarray(dataset.imagepath)\n","        self.class_arr = np.asarray(dataset.class_id)\n","        self.transform = transform\n","        self.data_len = len(dataset.index)\n","        \n","    def __len__(self):\n","        return self.data_len\n","    \n","    def __getitem__(self, index):\n","        \n","        ds = dcmread(self.imagearray[index])\n","        class_id = self.df.loc[index, 'class_id']\n","        arr = ds.pixel_array\n","        arr = np.stack((arr,)*3, axis=-1)\n","        arr = W_o_ten_transform(image = arr)[\"image\"]\n","        return arr, class_id\n","    \n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ChestData = MyDataset(df, transform=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ChestData_Aug = AlbumentationsDataset(df, transform=transform)\n","ChestData_Visual = VisualDataset(df, transform=transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["set_batchsize = 128"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader, Dataset, random_split\n","num_items = len(ChestData_Aug)\n","num_train = round(num_items * 0.7)\n","num_val = num_items - num_train\n","train_ds, val_ds = random_split(ChestData_Aug, [num_train, num_val])\n","train_dataloader = DataLoader(train_ds, batch_size=set_batchsize, num_workers=4, pin_memory=True, shuffle=True)\n","val_dataloader = DataLoader(val_ds,batch_size=set_batchsize, num_workers=4, pin_memory=True,shuffle=False)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision import models \n","import torch\n","model = models.resnet18(pretrained=True)\n","\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import nn as nn\n","num_classes = 15\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Linear(num_ftrs, num_classes)\n","device = torch.device(\"cuda:0\")\n","model= model.to(device)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.class_id.unique()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# torch.manual_seed(17)\n","\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import copy\n","\n","\n","def visualize_augmentations(dataset, idx=12,iterate='random', samples=9, cols=3):\n","    dataset = copy.deepcopy(dataset)\n","    dataset.transform = transform\n","    rows = samples // cols\n","    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 12))\n","    \n","    \n","    for i in range(samples):\n","        rando = random.randint(0,len(dataset)-1)\n","        if (iterate=='random'):\n","            image, _ = dataset[rando]\n","           \n","            ax.ravel()[i].imshow(image[:,:,0],cmap='gray')\n","            ax.ravel()[i].set_axis_off()\n","        else:\n","            image, _ = dataset[i]\n","            ax.ravel()[i].imshow(image[0,:,:],cmap='gray')\n","            ax.ravel()[i].set_axis_off()\n","            \n","    plt.tight_layout()\n","    filename = 'augmented_images_' + str(rando) + '.png'\n","    plt.savefig(filename)\n","    experiment.log_image(image_data = filename) \n","    plt.show()\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for t in range(9):\n","    visualize_augmentations(ChestData_Visual, idx=5, samples=9, cols=3)   \n","\n","# # %%\n","# for t in range(9):\n","#     visualize_augmentations(ChestData, idx=5, samples=9,iterate='iterate', cols=3)\n","# # %%\n","# random.seed(42)\n","# visualize_augmentations(ChestData)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def training(model, train_dataloader, num_epochs):\n","    optimizer_name = torch.optim.SGD(model.parameters(), lr=0.01)\n","    # criterion = nn.CrossEntropyLoss()\n","    criterion = nn.CrossEntropyLoss(weight=torch.tensor(pos_contribution).type(torch.FloatTensor).to(device))\n","    optimizer = optimizer_name\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n","                                                    max_lr=0.01,\n","                                                    steps_per_epoch=int(len(train_dataloader)),\n","                                                    epochs=num_epochs,\n","                                                    anneal_strategy='linear')\n","    \n","    for epoch in tqdm(range(num_epochs)):\n","        running_loss = 0.0\n","        correct_prediction = 0\n","        total_prediction = 0\n","        \n","        \n","        for i, data, in enumerate(tqdm(train_dataloader)):\n","            inputs = data[0].float().to(device)\n","            labels = data[1].to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            running_loss += loss.item()\n","            _, prediction = torch.max(outputs, 1)\n","            \n","            correct_prediction += (prediction == labels).sum().item()\n","            total_prediction += prediction.shape[0]\n","            running_acc = correct_prediction/total_prediction\n","            experiment.log_metric(\"Train/train_accuracy\", running_acc, epoch)\n","            try:\n","                experiment.log_metric(\"Loss/train\",running_loss/i, epoch)\n","            except:\n","                print('div by zero')\n","            if i > 2:\n","                if i % 20 == 0:\n","                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / i))\n","                \n","        num_batches = len(train_dataloader)\n","        avg_loss = running_loss / num_batches\n","        acc = correct_prediction/total_prediction\n","        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n","        experiment.log_metric(\"Accuracy\", acc, epoch)\n","        \n","num_epochs = 6 \n","training(model, train_dataloader, num_epochs)\n","experiment.end()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ----------------------------\n","# Inference\n","# ----------------------------\n","def inference (model, x):\n","  correct_prediction = 0\n","  total_prediction = 0\n","\n","  # Disable gradient updates\n","  with torch.no_grad():\n","    for data in x:\n","      # Get the input features and target labels, and put them on thresige GPU\n","      inputs = data[0].float().to(device)\n","      labels =  data[1].to(device)\n","      \n","      \n","      # Normalize the inputs\n","      inputs_m, inputs_s = inputs.mean(), inputs.std()\n","      inputs = (inputs - inputs_m) / inputs_s\n","\n","      # Get predictions\n","      outputs = model(inputs)\n","     \n","\n","      # Get the predicted class with the highest score\n","      _, prediction = torch.max(outputs,1)\n","      # Count of predictions that matched the target label\n","      correct_prediction += (prediction == labels).sum().item()\n","      total_prediction += prediction.shape[0]\n","      ic.ic(prediction)\n","    \n","  acc = correct_prediction/total_prediction\n","  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n","\n","# Run inference on trained model with the validation set\n","\n","inference(model, val_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import copy\n","def visualize_augmentations(dataset, idx=12, samples=9, cols=3):\n","    dataset = copy.deepcopy(dataset)\n","    dataset.transform = transform\n","    rows = samples // cols\n","    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 12))\n","    for i in range(samples):\n","        image, _ = dataset[i]\n","       \n","        ax.ravel()[i].imshow(image[0,:,:],cmap='gray')\n","        ax.ravel()[i].set_axis_off()\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random.seed(42)\n","visualize_augmentations(ChestData)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ds = dcmread(df.imagepath[5])\n","arr = ds.pixel_array\n","# arr = arr.astype('float')\n","arr = np.stack((arr,)*3, axis=-1)\n","\n","print(arr.shape)\n","print(arr.dtype)\n","\n","transformed = transform(image = arr)[\"image\"]\n","\n","def visualize(image):\n","    plt.figure(figsize=(10, 10))\n","    plt.axis('off')\n","    plt.imshow(image)\n","    \n","visualize(transformed)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}